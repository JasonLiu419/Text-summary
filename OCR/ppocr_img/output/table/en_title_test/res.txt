[641.0, 452.0, 1092.0, 452.0, 1092.0, 472.0, 641.0, 472.0]	('and process independent batches during training', 0.97106797)
[640.0, 478.0, 1091.0, 479.0, 1091.0, 501.0, 640.0, 500.0]	('phase. Two modes are available: synchronous', 0.9486082)
[639.0, 507.0, 1091.0, 508.0, 1091.0, 528.0, 639.0, 527.0]	('and asynchronous training. In synchronous train', 0.96087444)
[639.0, 534.0, 1090.0, 534.0, 1090.0, 557.0, 639.0, 557.0]	('ing, batches on parallel GPU are run simultane', 0.96262634)
[641.0, 565.0, 1093.0, 565.0, 1093.0, 586.0, 641.0, 586.0]	('ously and gradients aggregated to update master', 0.968496)
[639.0, 593.0, 1093.0, 592.0, 1093.0, 612.0, 639.0, 613.0]	('parametersbeforeresynchronizationoneachGPU', 0.9800649)
[639.0, 617.0, 1091.0, 622.0, 1091.0, 643.0, 639.0, 638.0]	('for the following batch. In asynchronous training', 0.9328016)
[639.0, 647.0, 1093.0, 650.0, 1093.0, 671.0, 639.0, 668.0]	('batches are run independent on each GPU and', 0.93667346)
[639.0, 676.0, 1094.0, 678.0, 1094.0, 698.0, 639.0, 696.0]	('independent gradients accumulated to the master', 0.9739829)
[640.0, 706.0, 1093.0, 704.0, 1093.0, 725.0, 640.0, 727.0]	('copy of the parameters. Asynchronous SGD is', 0.9683586)
[639.0, 732.0, 1090.0, 734.0, 1090.0, 755.0, 639.0, 753.0]	('known toprovide faster convergence Dean et al.', 0.9158357)
[642.0, 761.0, 1093.0, 761.0, 1093.0, 781.0, 642.0, 781.0]	('2012).Experiments with 8 GPUs show a 6x', 0.9502765)
[641.0, 790.0, 1092.0, 790.0, 1092.0, 810.0, 641.0, 810.0]	('speedupinperepoch, butaslight lossintraining', 0.9535343)
[642.0, 818.0, 1092.0, 818.0, 1092.0, 839.0, 642.0, 839.0]	('efficiency.Whentrainingtosimilarlossit givesa', 0.97981316)
[644.0, 849.0, 695.0, 849.0, 695.0, 860.0, 644.0, 860.0]	('3.5X 1', 0.87803245)
[696.0, 850.0, 728.0, 850.0, 728.0, 861.0, 696.0, 861.0]	('ota', 0.62192863)
[828.0, 854.0, 845.0, 854.0, 845.0, 861.0, 828.0, 861.0]	('1', 0.07172724)
[843.0, 851.0, 918.0, 851.0, 918.0, 862.0, 843.0, 862.0]	('training', 0.94625103)
[642.0, 894.0, 1025.0, 897.0, 1025.0, 917.0, 642.0, 914.0]	('C/Mobile/GPUTranslation Training', 0.95997125)
[1015.0, 898.0, 1089.0, 896.0, 1089.0, 913.0, 1015.0, 915.0]	('NMT', 0.9868875)
[641.0, 925.0, 1091.0, 925.0, 1091.0, 945.0, 641.0, 945.0]	('systems requires significant code complexity to', 0.9592962)
[639.0, 950.0, 1093.0, 951.0, 1093.0, 975.0, 639.0, 974.0]	('facilitate fast back-propagation-through-time. At', 0.9539194)
[641.0, 978.0, 1090.0, 978.0, 1090.0, 1001.0, 641.0, 1001.0]	('deployment, the system is much less complex', 0.9711008)
[639.0, 1006.0, 1091.0, 1007.0, 1091.0, 1031.0, 639.0, 1030.0]	('and only requires G) forwarding values through', 0.9631842)
[640.0, 1035.0, 1091.0, 1036.0, 1091.0, 1057.0, 640.0, 1056.0]	('the network and (ii) running a beam search thai', 0.9325128)
[640.0, 1063.0, 1091.0, 1064.0, 1091.0, 1085.0, 640.0, 1084.0]	('is much simplified compared to SMT. OpenNMT', 0.9738734)
[640.0, 1089.0, 1091.0, 1092.0, 1091.0, 1115.0, 640.0, 1112.0]	('includes several different translation deployments', 0.99273026)
[639.0, 1120.0, 1092.0, 1119.0, 1092.0, 1142.0, 639.0, 1143.0]	('specialized for different run-time environmentss', 0.9738936)
[639.0, 1145.0, 1093.0, 1149.0, 1093.0, 1172.0, 639.0, 1169.0]	('a batched CPU/GPU implementation for very', 0.9767667)
[639.0, 1175.0, 1089.0, 1176.0, 1089.0, 1200.0, 639.0, 1199.0]	('quickly translating a large set of sentences, 8', 0.9305268)
[641.0, 1204.0, 1092.0, 1205.0, 1092.0, 1226.0, 641.0, 1225.0]	('simple single-instance implementation for use', 0.9878108)
[640.0, 1232.0, 1090.0, 1232.0, 1090.0, 1255.0, 640.0, 1255.0]	('on mobile devices, and a specialized C imple', 0.962812)
[641.0, 1262.0, 1092.0, 1262.0, 1092.0, 1282.0, 641.0, 1282.0]	('mentation. The first implementation is suited', 0.9645331)
[640.0, 1289.0, 1092.0, 1290.0, 1092.0, 1311.0, 640.0, 1310.0]	('for research use, for instance allowing the user', 0.96668226)
[641.0, 1319.0, 1092.0, 1319.0, 1092.0, 1338.0, 641.0, 1338.0]	('to easily include constraints on the feasible set', 0.94513065)
[641.0, 1346.0, 1090.0, 1346.0, 1090.0, 1366.0, 641.0, 1366.0]	('of sentences and ideas such as pointer networks', 0.98124564)
[639.0, 1373.0, 1090.0, 1372.0, 1090.0, 1395.0, 639.0, 1396.0]	('and copy mechanisms. The last implementatior', 0.98477685)
[639.0, 1403.0, 1091.0, 1402.0, 1091.0, 1423.0, 639.0, 1424.0]	('is particularly suited for industrial use as it car', 0.96698725)
[640.0, 1432.0, 1092.0, 1432.0, 1092.0, 1452.0, 640.0, 1452.0]	('runon CPUinstandard production environments:', 0.9460163)
[639.0, 1459.0, 1092.0, 1460.0, 1092.0, 1480.0, 639.0, 1479.0]	('it reads the structure of the network and then', 0.9670359)
[640.0, 1488.0, 1092.0, 1487.0, 1092.0, 1507.0, 640.0, 1508.0]	('uses the Eigen package to implement the basic', 0.9669916)
[640.0, 1513.0, 1092.0, 1514.0, 1092.0, 1537.0, 640.0, 1536.0]	('linear algebra necessary for decoding. Table 4.1', 0.9555216)
[641.0, 1544.0, 1093.0, 1543.0, 1093.0, 1564.0, 641.0, 1565.0]	('compares the performance of the different', 0.9786712)
[645.0, 1578.0, 661.0, 1578.0, 661.0, 1585.0, 645.0, 1585.0]	('节', 0.23783395)
[149.0, 135.0, 604.0, 136.0, 604.0, 158.0, 149.0, 157.0]	('have been active development bythose outsideof', 0.9458662)
[149.0, 164.0, 603.0, 165.0, 603.0, 186.0, 149.0, 185.0]	('these twoorganizations.Theproject has an active', 0.9281037)
[149.0, 192.0, 601.0, 194.0, 601.0, 215.0, 149.0, 213.0]	('forumforcommunityfeedbackwithoverfivehun-', 0.9576799)
[151.0, 222.0, 602.0, 222.0, 602.0, 240.0, 151.0, 240.0]	('dredpostsinthelasttwomonths.Thereisalsoa', 0.96791)
[149.0, 249.0, 602.0, 252.0, 602.0, 270.0, 149.0, 267.0]	('live demonstration available of thesysteminuse', 0.93757635)
[151.0, 279.0, 243.0, 279.0, 243.0, 297.0, 151.0, 297.0]	('（Figure3)', 0.9253945)
[174.0, 306.0, 600.0, 306.0, 600.0, 327.0, 174.0, 327.0]	('One nice aspect of NMT asamodelisits rela', 0.9327559)
[150.0, 334.0, 599.0, 335.0, 599.0, 356.0, 150.0, 355.0]	('tive compactness. The Lua OpenNMT systemin', 0.93920314)
[149.0, 360.0, 601.0, 363.0, 601.0, 388.0, 149.0, 385.0]	('cluding preprocessing is roughly 4K lines of code', 0.9782831)
[150.0, 390.0, 600.0, 391.0, 600.0, 413.0, 150.0, 412.0]	('and the Python version is less than 1K lines Cali', 0.9339503)
[150.0, 418.0, 601.0, 419.0, 601.0, 441.0, 150.0, 440.0]	('though slightly less-feature complete). For com', 0.95655334)
[151.0, 448.0, 600.0, 448.0, 600.0, 469.0, 151.0, 469.0]	('parisonthe MosesSMTframeworkincludinglan', 0.95596087)
[149.0, 476.0, 602.0, 475.0, 602.0, 497.0, 149.0, 498.0]	('guage modeling is over 100K lines. This makes', 0.96115774)
[151.0, 504.0, 601.0, 504.0, 601.0, 525.0, 151.0, 525.0]	('thesystemeasytocompletelyunderstandfornew', 0.95819306)
[150.0, 533.0, 601.0, 533.0, 601.0, 553.0, 150.0, 553.0]	('comers. The project is fully self-contained de', 0.97012186)
[150.0, 560.0, 601.0, 557.0, 601.0, 579.0, 150.0, 582.0]	('pending on minimal number of external Lua li-', 0.9446287)
[150.0, 587.0, 600.0, 589.0, 600.0, 611.0, 150.0, 609.0]	('braries and including also a simple language int', 0.96196413)
[152.0, 619.0, 598.0, 619.0, 598.0, 637.0, 152.0, 637.0]	('dependentreversibletokenizationanddetokeniza', 0.9869943)
[152.0, 646.0, 236.0, 646.0, 236.0, 664.0, 152.0, 664.0]	('tiontools', 0.9083965)
[641.0, 319.0, 1093.0, 322.0, 1093.0, 340.0, 641.0, 336.0]	('Table1:Performancenumbersinsourcetokenspersecond', 0.9759507)
[639.0, 343.0, 1091.0, 342.0, 1091.0, 357.0, 639.0, 358.0]	('fortheTorchCPU/GPUimplementationsandforthemulti.', 0.97279906)
[638.0, 362.0, 1092.0, 362.0, 1092.0, 380.0, 638.0, 379.0]	('threadedCPUCimplementation.(RunWithINTel7/GTX', 0.9051993)
[640.0, 382.0, 683.0, 384.0, 682.0, 400.0, 639.0, 397.0]	('1080)', 0.90788954)
[150.0, 1047.0, 600.0, 1047.0, 600.0, 1067.0, 150.0, 1067.0]	('Memory Sharing When training GPU-basec', 0.9491323)
[149.0, 1073.0, 602.0, 1074.0, 602.0, 1094.0, 149.0, 1093.0]	('NMT models, memory size restrictions are the', 0.9813605)
[151.0, 1104.0, 599.0, 1104.0, 599.0, 1121.0, 151.0, 1121.0]	('mostcommonlimiterofbatchsizeand', 0.95142955)
[150.0, 1131.0, 599.0, 1131.0, 599.0, 1151.0, 150.0, 1151.0]	('rectlyimpacttrainingtime.Neuralnetworktoolk', 0.9772624)
[149.0, 1160.0, 602.0, 1159.0, 602.0, 1179.0, 149.0, 1180.0]	('its,suchas Torch, are often designed totrade-off', 0.9559689)
[152.0, 1188.0, 600.0, 1188.0, 600.0, 1208.0, 152.0, 1208.0]	('extra memory allocations for speed and declar', 0.9749635)
[150.0, 1214.0, 602.0, 1216.0, 602.0, 1237.0, 150.0, 1235.0]	('ative simplicity. For OpenNMT, we wanted to', 0.95920795)
[149.0, 1241.0, 601.0, 1243.0, 601.0, 1266.0, 149.0, 1264.0]	('haveit bothwaysandsoweimplemented anex-', 0.96483916)
[149.0, 1270.0, 602.0, 1271.0, 602.0, 1295.0, 149.0, 1294.0]	('ternal memory sharing system that exploits the', 0.96271193)
[149.0, 1298.0, 601.0, 1300.0, 601.0, 1321.0, 149.0, 1319.0]	('known time-series control flow of NMT systems', 0.9681113)
[150.0, 1329.0, 599.0, 1329.0, 599.0, 1349.0, 150.0, 1349.0]	('and aggressively shares the internal buffers be', 0.9772178)
[150.0, 1355.0, 599.0, 1355.0, 599.0, 1379.0, 150.0, 1379.0]	('tween clones.The potential shared buffers are dy', 0.9425316)
[150.0, 1386.0, 600.0, 1386.0, 600.0, 1406.0, 150.0, 1406.0]	('namicallycalculatedbyexplorationofthenetwork', 0.9775875)
[150.0, 1413.0, 600.0, 1413.0, 600.0, 1437.0, 150.0, 1437.0]	('graph before starting training.In practical use, ag', 0.9330279)
[149.0, 1442.0, 600.0, 1441.0, 600.0, 1462.0, 149.0, 1463.0]	('gressive memory reuse provides a saving of 70%', 0.96135455)
[322.0, 1472.0, 550.0, 1472.0, 550.0, 1486.0, 322.0, 1486.0]	('iththedefaultmodelsize', 0.9861311)
[150.0, 1515.0, 603.0, 1518.0, 603.0, 1538.0, 150.0, 1536.0]	('Multi-GPU OpenNMT additionally supports', 0.95448196)
[150.0, 1544.0, 602.0, 1544.0, 602.0, 1565.0, 150.0, 1565.0]	('multi-GPU training using data parallelism.Each', 0.953467)
[152.0, 1572.0, 253.0, 1574.0, 252.0, 1589.0, 152.0, 1587.0]	('GPU HAS', 0.73346376)
[261.0, 1575.0, 343.0, 1575.0, 343.0, 1589.0, 261.0, 1589.0]	('Lreplica', 0.92018294)
[426.0, 1575.0, 491.0, 1575.0, 491.0, 1589.0, 426.0, 1589.0]	('master', 0.97290856)
[508.0, 1577.0, 602.0, 1577.0, 602.0, 1589.0, 508.0, 1589.0]	('barameters', 0.83458483)
[151.0, 913.0, 602.0, 915.0, 602.0, 938.0, 151.0, 937.0]	('As NMT systems can take from days to weeks to', 0.9626288)
[150.0, 942.0, 601.0, 946.0, 601.0, 967.0, 150.0, 963.0]	('train, training efficiency is a paramount concern', 0.95462704)
[151.0, 971.0, 603.0, 971.0, 603.0, 994.0, 151.0, 994.0]	('Slightly faster training can make be the difference', 0.9692749)
[234.0, 1007.0, 355.0, 1007.0, 355.0, 1015.0, 234.0, 1015.0]	('Dasiean0', 0.461469)
[499.0, 1007.0, 569.0, 1007.0, 569.0, 1015.0, 499.0, 1015.0]	('mments', 0.76397926)
[282.0, 701.0, 319.0, 701.0, 319.0, 714.0, 282.0, 714.0]	('roal', 0.66669446)
[151.0, 743.0, 599.0, 744.0, 599.0, 762.0, 151.0, 761.0]	('Asthe low-leveldetails ofNMThavebeen cov', 0.96102124)
[151.0, 772.0, 600.0, 773.0, 600.0, 791.0, 151.0, 790.0]	('eredpreviously,wefocusthisreportonthedesigr', 0.97132003)
[151.0, 800.0, 600.0, 800.0, 600.0, 821.0, 151.0, 821.0]	('goalsofOpenNMT:systemefficiency,', 0.9549618)
[151.0, 828.0, 433.0, 829.0, 433.0, 847.0, 151.0, 847.0]	('ularityandmodlextensibility', 0.8270062)
